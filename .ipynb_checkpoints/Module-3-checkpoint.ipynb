{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe8d766a-033a-4e8a-a42a-5d401f069f39",
   "metadata": {},
   "source": [
    "# Thread level Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ece93-fce1-4179-b3cf-f8c6bda9cc82",
   "metadata": {},
   "source": [
    "#### **1. Checkpointers (in LangGraph)**\n",
    "\n",
    "* Think of a **checkpointer** like a “save game” in a video game.\n",
    "* It **remembers the state** of your workflow (like what data you’ve processed, what step you’re on).\n",
    "* If something crashes or you want to resume later, the checkpointer lets you **continue from where you left off** instead of starting from scratch.\n",
    "\n",
    "Example:\n",
    "You’re processing 100 documents. After 50, your program stops.\n",
    "\n",
    "* With a checkpointer → next time, it resumes at document 51.\n",
    "* Without a checkpointer → it starts again from document 1.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Threads (in LangGraph)**\n",
    "\n",
    "* A **thread** is like a “conversation line” or “execution flow.”\n",
    "* Each user session or workflow run can have its own **thread ID**.\n",
    "* This keeps data and context separate between different tasks or users.\n",
    "\n",
    "Example:\n",
    "\n",
    "* User A’s chatbot session = Thread 1\n",
    "* User B’s chatbot session = Thread 2\n",
    "* They don’t mix, because each thread has its own memory and checkpoints.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. State Snapshot**\n",
    "\n",
    "* A **snapshot** is like taking a photo of your program’s current state.\n",
    "* It freezes everything (inputs, outputs, progress) so you can **restore it later**.\n",
    "* Snapshots are created by checkpointers to know exactly where to restart.\n",
    "\n",
    "Example: Chatbot saves a snapshot after every reply. If it crashes, it reloads the last snapshot and continues.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Short-term Memory**\n",
    "\n",
    "* Short-term memory = what the system remembers **within a single thread/session**.\n",
    "* It’s like remembering the last few messages, but it doesn’t survive forever.\n",
    "\n",
    "Example: The chatbot remembers what you said 2 minutes ago in the same session.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Long-term Memory**\n",
    "\n",
    "* Long-term memory = what’s saved **permanently across sessions/threads**.\n",
    "* It survives restarts and can be recalled even in a new conversation.\n",
    "\n",
    "Example: The chatbot remembers your name and preferences even if you come back tomorrow.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2de003-2093-4607-ae00-8c1551073bbd",
   "metadata": {},
   "source": [
    "# Short-term memory with the InMemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5163238-a891-4157-952c-a658f8d967bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage, RemoveMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e235a3-f23e-4b24-a699-8d3956b46078",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=config.gemini_api_key,\n",
    "    temperature=0,\n",
    "    max_tokens=250,\n",
    "    model_kwargs={\"seed\": 42}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1a6233-3ef7-47d1-8821-4480f000b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AskGeminiAI(question):\n",
    "    response = llm.invoke(question)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b81df8-8598-44a2-85c2-e81325954cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3f17f8-3916-4572-8d28-f085b0f9542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(state: State) -> State:\n",
    "    \n",
    "    print(f\"\\n-------> ENTERING ask_question:\")\n",
    "    \n",
    "    question = \"What is your question?\"\n",
    "    print(question)\n",
    "    \n",
    "    return State(messages = [AIMessage(question), HumanMessage(input())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f718052-ea57-46d8-bc12-988164479e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State) -> State:\n",
    "    \n",
    "    print(f\"\\n-------> ENTERING chatbot:\")\n",
    "        \n",
    "    system_message = f'''\n",
    "    Here's a quick summary of what's been discussed so far:\n",
    "    {state.get(\"summary\", \"\")}\n",
    "    \n",
    "    Keep this in mind as you answer the next question.\n",
    "    '''\n",
    "    \n",
    "    response = AskGeminiAI([SystemMessage(system_message)] + state[\"messages\"])\n",
    "    response.pretty_print()\n",
    "    \n",
    "    return State(messages = [response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1bd4387-ca86-4eeb-be40-86fbe8c413d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_messages(state: State) -> State:\n",
    "    print(f\"\\n-------> ENTERING summarize_messages:\")\n",
    "\n",
    "    new_conversation = \"\"\n",
    "    for i in state[\"messages\"]:\n",
    "        new_conversation += f\"{i.type}: {i.content}\\n\\n\"\n",
    "    \n",
    "    summary_instructions = f'''\n",
    "        Update the ongoing summary by incorporating the new lines of conversation below. \n",
    "        Build upon the previous summary rather than repeating it, \n",
    "        so that the result reflects the most recent context and developments.\n",
    "        Respond only with the summary.\n",
    "        \n",
    "        Previous Summary:\n",
    "        {state.get(\"summary\", \"\")}\n",
    "        \n",
    "        New Conversation:\n",
    "        {new_conversation}\n",
    "        '''\n",
    "\n",
    "    print(summary_instructions)\n",
    "\n",
    "    summary = AskGeminiAI([HumanMessage(summary_instructions)])\n",
    "    \n",
    "    remove_messages = [RemoveMessage(id = i.id) for i in state[\"messages\"][:]]\n",
    "\n",
    "    return State(messages = remove_messages, summary = summary.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e7723a5-6253-43ec-878a-aca55b817f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x27486788750>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_node(\"ask_question\", ask_question)\n",
    "graph.add_node(\"chatbot\", chatbot)\n",
    "graph.add_node(\"summarize_messages\", summarize_messages)\n",
    "\n",
    "graph.add_edge(START, \"ask_question\")\n",
    "graph.add_edge(\"ask_question\", \"chatbot\")\n",
    "graph.add_edge(\"chatbot\", \"summarize_messages\")\n",
    "graph.add_edge(\"summarize_messages\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad6ea384-db25-4114-864e-59eb1a9f9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = InMemorySaver()\n",
    "graph_compiled = graph.compile(checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae27f8-754e-40ed-bea3-4f46eee37bcc",
   "metadata": {},
   "source": [
    "1. **`InMemorySaver()`**\n",
    "\n",
    "   * This is your **checkpointer**.\n",
    "   * It stores the graph’s progress (state, snapshots) **in memory (RAM)**.\n",
    "   * So if you run the graph again in the same session, it can remember where it left off.\n",
    "   * But once you stop the program, this memory is gone (not saved to disk).\n",
    "\n",
    "2. **`graph.compile(checkpointer)`**\n",
    "\n",
    "   * This takes your `graph` and **adds checkpointing ability** to it.\n",
    "   * So every time the graph runs, it can **save and reload snapshots** using the checkpointer.\n",
    "\n",
    "---    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83c22b1e-88e0-4673-ae11-830bffa0d8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    +-----------+      \n",
      "    | __start__ |      \n",
      "    +-----------+      \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "   +--------------+    \n",
      "   | ask_question |    \n",
      "   +--------------+    \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "      +---------+      \n",
      "      | chatbot |      \n",
      "      +---------+      \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "+--------------------+ \n",
      "| summarize_messages | \n",
      "+--------------------+ \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "      +---------+      \n",
      "      | __end__ |      \n",
      "      +---------+      \n"
     ]
    }
   ],
   "source": [
    "print(graph_compiled.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a19bb142-b643-4db5-84a5-b2e810150a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "config2 = {\"configurable\": {\"thread_id\": \"2\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ab1e7-ee75-4146-8cf1-d67bca2ebd39",
   "metadata": {},
   "source": [
    "### What this does\n",
    "\n",
    "1. **`config1` and `config2`**\n",
    "\n",
    "   * These are **thread configurations**.\n",
    "   * Each has a different `thread_id` (`\"1\"` vs `\"2\"`).\n",
    "   * That means LangGraph will treat them as **two separate conversation flows**.\n",
    "\n",
    "2. **`graph_compiled.invoke(State(), config2)`**\n",
    "\n",
    "   * You’re starting the graph with an initial `State()` in **thread 2**.\n",
    "   * Because the `thread_id` is `2`, LangGraph will **store and checkpoint progress** separately from thread 1.\n",
    "   * If you invoked with `config1`, it would continue the progress of **thread 1**, not thread 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c61b08d-d24d-4418-b831-ad85d136c901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------> ENTERING ask_question:\n",
      "What is your question?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " My pet name is Sonu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------> ENTERING chatbot:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Okay, I will remember that your pet name is Sonu.\n",
      "\n",
      "-------> ENTERING summarize_messages:\n",
      "\n",
      "        Update the ongoing summary by incorporating the new lines of conversation below. \n",
      "        Build upon the previous summary rather than repeating it, \n",
      "        so that the result reflects the most recent context and developments.\n",
      "        Respond only with the summary.\n",
      "\n",
      "        Previous Summary:\n",
      "        \n",
      "\n",
      "        New Conversation:\n",
      "        ai: What is your question?\n",
      "\n",
      "human: My pet name is Sonu.\n",
      "\n",
      "ai: Okay, I will remember that your pet name is Sonu.\n",
      "\n",
      "\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [],\n",
       " 'summary': 'The AI is interacting with a user and has asked a question. The user responded by stating their pet name is Sonu, and the AI has acknowledged this and stated it will remember the information.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_compiled.invoke(State(), config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10080ace-4d3d-4705-a6ee-0577615c16ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------> ENTERING ask_question:\n",
      "What is your question?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " What is my pet name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------> ENTERING chatbot:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your pet name is Sonu.\n",
      "\n",
      "-------> ENTERING summarize_messages:\n",
      "\n",
      "        Update the ongoing summary by incorporating the new lines of conversation below. \n",
      "        Build upon the previous summary rather than repeating it, \n",
      "        so that the result reflects the most recent context and developments.\n",
      "        Respond only with the summary.\n",
      "\n",
      "        Previous Summary:\n",
      "        The AI is interacting with a user and has asked a question. The user responded by stating their pet name is Sonu, and the AI has acknowledged this and stated it will remember the information.\n",
      "\n",
      "        New Conversation:\n",
      "        ai: What is your question?\n",
      "\n",
      "human: What is my pet name?\n",
      "\n",
      "ai: Your pet name is Sonu.\n",
      "\n",
      "\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [],\n",
       " 'summary': \"The AI is interacting with a user and has asked a question. The user responded by stating their pet name is Sonu, and the AI has acknowledged this and stated it will remember the information. The AI then asked the user what their question was, and the user asked what their pet name was. The AI then correctly stated the user's pet name is Sonu.\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_compiled.invoke(State(), config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad2fc00a-fa77-43ca-9eb5-22d75621a479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------> ENTERING ask_question:\n",
      "What is your question?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Do you know my pet name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------> ENTERING chatbot:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "As a large language model, I have no memory of past conversations and no personal connection to you. Therefore, I do not know your pet's name.\n",
      "\n",
      "-------> ENTERING summarize_messages:\n",
      "\n",
      "        Update the ongoing summary by incorporating the new lines of conversation below. \n",
      "        Build upon the previous summary rather than repeating it, \n",
      "        so that the result reflects the most recent context and developments.\n",
      "        Respond only with the summary.\n",
      "\n",
      "        Previous Summary:\n",
      "        \n",
      "\n",
      "        New Conversation:\n",
      "        ai: What is your question?\n",
      "\n",
      "human: Do you know my pet name?\n",
      "\n",
      "ai: As a large language model, I have no memory of past conversations and no personal connection to you. Therefore, I do not know your pet's name.\n",
      "\n",
      "\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [],\n",
       " 'summary': \"The AI has stated it has no memory of past conversations and no personal connection to the user. Consequently, it does not know the user's pet's name.\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_compiled.invoke(State(), config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad4f68-cf6c-4cf4-86ff-d1263d7852ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llms_course_env)",
   "language": "python",
   "name": "llms_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
